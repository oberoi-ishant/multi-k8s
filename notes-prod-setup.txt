Navigate to console.cloud.google.com

Directions for cleaning up Google Cloud cluster
https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/v4/t/lecture/11684242?start=0

https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11628212#questions/9889368/
1. Create account
2. Create project
3. From left -> Kubernetes Engine
4 Create Cluster
  name: mutli-cluster
  no of nodes: 3
  node type: vCPU standard 3.67GB
  Create.

Check the Image: travis-yml-for-gcp-deployment.png file for steps to up the
.travis.yml file for production.

Check IMage: gen-serivce-account.png, create-service-account.png for steps to generate a service aacount.
1. console.cloud.google.com
2. IAM and Admin
  Service Account -> Create Service Account
  account name and id: travis-deployer
  Role/Permission -> Kubernetes Engine Admin -> Create Key: Json
  Now the json file is automaticaaly downloaded in your computer.
  Keep it safe and do not push to it or expose outside.

Install travis CI:
Link: https://github.com/travis-ci/travis.rb
Requirement ruby should be installed. In macOS ruby is installed by default.
So for windows user, either install ruby or install a docker image that has ruby pre-installed
and then install travis-cli in there.
Image(travis-install-encrypt.png) for steps both for windows n mac.
Commands:
1. docker run -it -v $(pwd):/app ruby:2.3 sh (Get an image with ruby and run shell inside the container)
2. now if you do cd app/ then ls, you see our project directories client,  server, worker because of volume we setup -v $(pwd):/app
3. install travis-cli using gem. Gem is dependency manager for ruby language.
4. gem install travis -v 1.8.10
5. We did not use the alpine version of ruby as alpine version is bare minimum and we could not use it for our purpose(gem install some packages).
6. then run: travis and all available travis cli commands will be listed.
7. travis login (if issue travis login --org or --com)
8. enter githhub credentials. (if issues first login using travis-ci.com on browser then try again. Travis is miragting from .org to .com so some issue here in accounts.)
9. copy the downloaded json file from serice account into the current directoy (pwd),
   as this is the volumewe setup uising -v $(pwd):/app
10. Rename it to service-account.json, as this is the name we have given in .travis.yml
11. Verify in container, in app/ this file is now present.
12. Encrypt file: -r is the repository name on travis-ci. you can go to travis in browser and see the name of repo.
travis encrypt-file service-account.json -r oberoi-ishant/multi-k8s
13. Add the output openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d to the
command list in before_install in .travis.yml.
before_install:
- openssl aes-256-cbc -K $encrypted_0c35eebf403c_key -iv $encrypted_0c35eebf403c_iv -in service-account.json.enc -out service-account.json -d
Doing this decrypts the file and places the decrypted service-account.json in the same folder so that
gcloud auth activate-service-account --key-file service-account.json can run safely.
14. Make sure to add service-account.json.enc to the git repository.
Make sure NOT to add service-account.json to the git repository,
So you can delete it or copy it somewhere else.
Commit changes to .travis.yml

Note travis does not have a built in provider for kubernetes deployment, hence we cannot
simply add a deploy section and mention provider: elasticbeanstalk like we mentioned in
aws project. So will write a custom script for deployment.
So will say provider: script.

# Tag images with latest version
# Command to update an OBJECT inside the cluster
# kubectl set <property> <object-type>/<object-name> <container-name>=<new-property>

# Note By default if you do  not specify the version number of image
# to use, it is consider as lastest. So
# kubectl set image deployment/server-deployment server=ishantoberoi/multi-server
# is equal to
# kubectl set image deployment/server-deployment server=ishantoberoi/multi-server:latest
# So k8s things it is already running latest version and there is nothing to update.
# Image (tag-image-latest-issue.png)
# So will use dynamic tagging of images using git sha.
# Get current git sha:  git rev-parse HEAD
# SHA is unique id of commit ~ commit id
# Image(solve-tag-image-git-sha.png)
# Image (why-use-tag-latest.png)
# So we will tag image with two tags:
# one :latest,so if new engineer clones project and runs kubectl apply -f k8s,
# he gets all the latest images as :latest tag is implicit in the deployment files
# their image: ishantoberoi/multi-client means ishantoberoi/multi-client:latest.
# second: with git sha to push the new image with new commit id to give a unique name, so that k8s can update
# the deployment.
# With this approcah we have two benefits,
# latest image remains updated as latest
# and codewise we have an image per commit id, so helpful in debugging also
# We can see what image:commit-id is running on prod, checkout that git commit and debug locally.

# taggging with sha helps get image a new tag so that k8s can update the
# deployment with new image.


GENERATING A SECRET ON GOOGLE CLOUD (for injecting passwords)
Image(activate-cloud-shell-gcloud.png, gcloud-shell.png)
1. Activate google cloud shell
2. Run the commands: as put in travis.yml (Image: gcloud-shell-commands.png) for
  every new cluster you create. Once per cluster.
  # set the project id on gcloud
  - gcloud config set project multi-k8s-274912 # this is the project ID in google cloud. Image(multi-k8s-id.png)
  # set the location on gcloud
  - gcloud config set compute/zone asia-east1-a	 # location of your multi-cluster in google cloud. (Image: gcloud-cluster-location)
  # set the container name on gcloud
  - gcloud container clusters get-credentials multi-cluster # name of the cluster on google cloud. (Image: gcloud-cluster-location)

  Test now if you are connected to the production cluster on google cloud.
  Just run simple, kubectl get pods in cloud shell, if it says No resources found, means we are good, Connnected.

  Now create secret on gcloud by running same kubectl command
  - kubectl create secret generic pgpassword --from-literal PGPASSWORD=pgpassword
  Make sure secret name(pgpassword) and key(PGPASSWORD) match what is give in deployment files(postgres-deployment, server-deployment).
  actuall (PGPASSWORD=pgpassword => key=value) value (here pgpassword) can be different from the dev password.

  Now refresh page and in Configuration tab you should see the secret created.
  Image(verify-secret-created-google-cloud.png)

Install ingress-nginx Using Helm: (https://kubernetes.github.io/ingress-nginx/deploy/#using-helm)
Helm: (kubernetes-package-manager) a program that helps to administer third party softwares inside of the k8s cluster.
So we can install some third party softwares inside the k8s cluster using helm.
Much like brew for mac.
Link: github.com/helm/helm, https://helm.sh/, https://helm.sh/docs/intro/quickstart/
Helm combines two softwares - Helm + Tiller
So we issue command to Helm Client -> Tiller Server.
Tiller server will be running inside the k8s cluster after installation
and will help to install softwares. Muck like docker-client + docker-server.
Image(helm-tiller.png)
Helm has a lot of options of installation like from brew etc.
But we do not have brew on our production cluster, so will use the mentioned Script method.

We are using helm 2. There is also a helm 3 version.
Run this on the google cloud shell
- curl -LO https://git.io/get_helm.sh
- chmod 700 get_helm.sh
- ./get_helm.sh

Note: helm has a new version 3. This is without Tiller. Here I am following
the helm 3 approach.

RBAC
In minikube env there is no RBAC enabled hence we were good.
In Google Cloud RBAC is enabled by default, so Tiller needs some Permission to
make changes to the cluster running on prod.
Since helm 3 has no tiller there is differentset of instructions.
We use helm 2 here, and follow the instructor.

Tiller in the following lectures. Image(helm-3-process.png)
Image: k8s-security-with-rbac.png
User Account: identifies a person.
Service Account: identifies a pod.
They are like authenticating.
ClusterRoleBinding: Authorizes an account (user/serivce) to do a certain set of actions
across the ENITRE cluster.
RoleBinding: Authorizes an account (user/serivce) to do a certain set of actions
in a single NAMESPACE.
kubectl get namespace.
type of Object.

(To be run with helm 2, to grant Permission to tiller in the gcloud shell)
Helm 2 setup: (gcloud shell)
- curl -LO https://git.io/get_helm.sh
- chmod 700 get_helm.sh
- ./get_helm.sh
Creating a Service Account:
- kubectl create serviceaccount --namespace kube-system tiller
  create a serviceaccount with name tiller and add it to the kube-system namespace
  so we do not accidently do any changes to the account.
  check: kubect get namespace for type of namespaces.
- kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
  create a clusterrolebinding and give all cluster permissions using --clusterrole=cluster-admin and add it to the serviceaccount tiller in kube-system namespace using kube-system:tiller
- helm init --service-account tiller --upgrade
  (now install nginx-ingress using helm 2)
- helm install stable/nginx-ingress --name my-nginx --set rbac.create=true
https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11628280#questions

Now in the Workloads we should see
1. my-nginx-nginx-ingress-controller
2. my-nginx-nginx-ingress-default-backend

And in Services and Ingress
1. my-nginx-nginx-ingress-controller (Load balancer)
2. my-nginx-nginx-ingress-default-backend (ClusterIP for backend)
Note: We should see two endpoints in my-nginx-nginx-ingress-controller,
one for ports 80 and one for port 443. If you check this IP on browseryou see a default
404 page. This is also provided by default-backend created, along with some health check.

Also in Network Service -> Load Balancing -> you should see the load balancer
created as shown in Image(load-balancer-details, load-balancer-gcloud).
This is the load balancer shown in prod setup Image nginx-ingress-google-cloud-setup.png
So this is having the same IP we saw above in endpoint and is governing
access to our 3 NODES on production.
This load balancer directs traffic to load balancer service (which is k8s type of service)
and which directs traffic to the nginx-ingress controller which then directs traffic to our
deployments.

Now Git push all changes and see build trigger and pass in travis ci.
If success then go to the load balancer IP and you should see APP LIVE.
Also you should see all deployments mentioned in Workloads and Services created on google Cloud console.


Now process to make a change to code and push it to production.
image(workflow-changes-in-production.png)


Setting up https on k8s
Image(https-k8s-overview.png)
1. Get a domain.
  In Domain settings DNS
  Add/Edit (In godaddy DNS i did it)
  Type A account to point to the load balancer IP endpoint
  Add a record for www and point to your domain name ishantoberoi.com

2.Project used https://github.com/jetstack/cert-manager
Follow instructions here to install cert manager using Helm 2
https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/16924480#questions
Run command in gcloud shell.
1.Apply the yml file
kubectl apply --validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.11/deploy/manifests/00-crds.yaml
2. Create the namespace for cert-manager
kubectl create namespace cert-manager
3. Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io
4. Update your local Helm chart repository cache
helm repo update
5. Install the cert-manager Helm chart:
helm install \
  --name cert-manager \
  --namespace cert-manager \
  --version v0.11.0 \
  jetstack/cert-manager

Verify by running kubectl get pods --namespace cert-manager.
You should see the cert-manager, cert-manager-cainjector, and cert-manager-webhook pod in a Running state.

We are using service called https://letsencrypt.org/ to get the Certificate.
Now steps to wire cert manager (Image: wire-cert-manager.png)
So will create two objects
1. Issuer - tells cert manager where to get Certificate from.
2. Certificate - object describing details of the Certificate that should be obtained.

Not commit code and push to master branch. Build pass.
Then check in your gcloud shell
kubectl get certficate.
- ishantoberoi-com-tls should be present.
Or kubectl describe certficate
Output: something like:
Events:
  Type    Reason        Age   From          Message
  ----    ------        ----  ----          -------
  Normal  GeneratedKey  16m   cert-manager  Generated a new private key
  Normal  Requested     16m   cert-manager  Created new CertificateRequest resource "ishantoberoi-com-tls-2956046638"
  Normal  Issued        16m   cert-manager  Certificate issued successfully

And secret created using -
 kubectl get secret / /Configuration tab
Output
NAME                                         TYPE                                  DATA   AGE
ishantoberoi-com                             kubernetes.io/tls                     3      17m
as specified in certificate.yml

Now when certificate will expire in future, certmanager will attempt the entire
process of getting new certificate all again.


Just saving here.
ingress-service.yml
Rule section before https config.
  rules:
    - http:
        paths:
          - path: /?(.*) # this will help us save from the error we had see in past this.seenIndexes.map is not a function.
            backend:
              serviceName: client-cluster-ip-service
              servicePort: 3000
          - path: /api/?(.*) # this will help us save from the error we had see in past this.seenIndexes.map is not a function.
            backend :
              serviceName: server-cluster-ip-service
              servicePort: 5000
Make changes in the ingress-service.yml file for https.
git add, commit and push master.